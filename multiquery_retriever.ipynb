{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本文讲述了如何通过Langchain的MultiQueryRetriever、LongContextReorder和ContextualCompression技术改善RAG效果，\n",
    "# 减少无关文档干扰，提升大型语言模型的提问与总结能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、Multi Query Retriever[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a sample vectorDB\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import StrOutputParser\n",
    "import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "# load blog post\n",
    "loader = WebBaseLoader(\"https://www.investopedia.com/terms/c/capital-asset-pricing-model.asp\")\n",
    "data = loader.load()\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# vectorDB\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare multi query retriever\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectordb.as_retriever(), llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup QnA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_system_prompt = \"\"\"\n",
    "    Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    {context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def format_docs (docs):\n",
    "    doc_strings = [doc.page_content for doc in docs]\n",
    "    return \"\\n\\n\".join(doc_strings)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_from_llm | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、Long Context Reorder[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 长上下文重排序非常适合于我们想要从矢量数据库返回10个以上文档的情况。\n",
    "\n",
    "# QnA Part and Reordering\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    # call the reordering function in here\n",
    "    reordering = LongContextReorder()\n",
    "    reordered_docs = reordering.transform_documents(docs)\n",
    "    doc_strings = [doc.page_content for doc in reordered_docs]\n",
    "    return \"\\n\\n\".join(doc_strings)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": vectordb.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    } \n",
    "    | qa_prompt\n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、Contextual Compression[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个块长度很长，且包含有不相干的信息；使用LLM从检索到的文档中删除那些不相关的段落。\n",
    "# contextual compression setup\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline, EmbeddingsFilter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings,similarity_threshold=0.9)\n",
    "pipeline_compressor = DocumentCompressorPipeline(transformers=[splitter, redundant_filter, relevant_filter])\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    doc_strings = [doc.page_content for doc in docs]\n",
    "    return \"\\n\\n\".join(doc_strings)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": compression_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What did the author do growing up?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
