{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建完整的RAG应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1 搜集文献资料 pdf\n",
    "## 分类保存在文件夹中，构建文件夹树形结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./papers\\\\Enantioselective Iridium-Catalyzed Allylic Substitution with 2-Methylpyridines.pdf',\n",
       " './papers\\\\Iridium-Catalyzed Asymmetric Allylic Amination Reaction with N-Aryl Phosphoramidite Ligands.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_file_dirs(directory):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                fp = os.path.join(root, file)\n",
    "                file_paths.append(fp)\n",
    "    return file_paths\n",
    "\n",
    "root_dir = \"./papers\"\n",
    "files = extract_file_dirs(root_dir)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文件由pdf转换为txt，保存在当前目录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# 将从pdf中提取的文字写入文件\n",
    "def write_to_file(text, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "docs = []\n",
    "docs.append(extract_text(files[0]))\n",
    "docs\n",
    "\n",
    "# 循环调用函数\n",
    "for i in range(0, len(docs)):\n",
    "    # 判断是否存在文件夹，若不存在则创建\n",
    "    if not os.path.exists('txt_papers'):\n",
    "        os.mkdir('txt_papers')\n",
    "    file_name = f'txt_papers/text_{i}.txt'\n",
    "    write_to_file(docs[i], file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step2 切分文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Communications\\n\\nAngewandte\\n\\nChemie\\n\\nAsymmetric Catalysis\\n\\nInternational Edition: DOI: 10.1002/anie.201700433\\nGerman Edition:\\nDOI: 10.1002/ange.201700433\\n\\nEnantioselective Iridium-Catalyzed Allylic Substitution with\\n2-Methylpyridines\\nXi-Jia Liu and Shu-Li You*\\n\\nAbstract: An enantioselective iridium-catalyzed allylic sub-\\nstitution with a set of highly unstabilized nucleophiles gen-\\nerated in situ from 2-methylpyridines is described. Enantioen-\\nriched 2-substituted pyridines, which are frequently encoun-\\ntered in natural products and pharmaceuticals, could be easily\\nconstructed by this simple method in good yields and excellent\\nenantioselectivity. The synthetic utility of the pyridine products\\nis demonstrated through the synthesis of a key intermediate of\\na reported Na+/H+ exchanger inhibitor and the total synthesis\\nof ((cid:2))-lycopladine A.\\n\\nPyridines are among the most prevalent heterocyclic',\n",
       "  'Pyridines are among the most prevalent heterocyclic\\n\\nstructural moieties in biologically active natural products,\\npharmaceuticals, and agrochemicals.[1] For example, twelve\\nsmall molecules containing a pyridine motif were listed in the\\ntop 200 pharmaceutical products in 2012, including the top\\nentry, Nexium.[2] Therefore, diverse functionalization of\\npyridine derivatives could lead to wide applications in many\\nfields.[3] One aspect of our ongoing efforts to develop methods\\nto build complex and biologically active molecules is the\\napplication of iridium-catalyzed allylic substitution reactions\\nwith heteroaromatic compounds as nucleophiles.[4]\\n\\nIridium-catalyzed enantioselective allylic substitution\\nreactions have become a powerful tool for constructing\\ncarbon–heteroatom and carbon–carbon bonds, and they have\\nfound extensive application in the total synthesis of natural\\nproducts and the preparation of bioactive compounds.[5]\\nAlthough a wide range of nucleophiles can be employed in\\nthese reactions, unstabilized carbon-type nucleophiles have\\nbeen much less studied and have been mainly limited to\\nenolates, organoborons, and aryl- or alkylzinc bromides[6]\\nowing to challenges relating to reactivity and controlling the\\nregio- and enantioselectivity. The exploration of unstabilized\\nnucleophiles in iridium-catalyzed asymmetric allylic substitu-\\ntion is thus still of great importance.\\n\\nRecently,',\n",
       "  'Recently,\\n\\ntransition-metal-catalyzed allylic alkylation\\nreactions of unstabilized benzylic nucleophiles have attracted\\n\\n[*] X.-J. Liu, Prof. Dr. S.-L. You\\n\\nState Key Laboratory of Organometallic Chemistry\\nShanghai Institute of Organic Chemistry\\nChinese Academy of Sciences\\n345 Lingling Lu, Shanghai 200032 (China)\\nE-mail: slyou@sioc.ac.cn\\nHomepage: http://shuliyou.sioc.ac.cn/\\n\\nProf. Dr. S.-L. You\\nCollaborative Innovation Center of Chemical Science and\\nEngineering, Tianjin (China)\\n\\nSupporting information and the ORCID identification number(s) for\\nthe author(s) of this article can be found under:\\nhttp://dx.doi.org/10.1002/anie.201700433.',\n",
       "  'interest because of their applications in the synthesis of\\na broad diversity of complex molecules.[7] Pioneering works\\nfrom the groups of Trost[7a] and Walsh[7h] have led to\\nimpressive palladium-catalyzed processes for asymmetric\\nallylic alkylation with 2-methylpyridine derivatives (pKa\\n(cid:3) 34[8a]) and toluene derivatives (pKa (cid:3) 44[8b]), respectively.\\nIn these studies, elegant strategies have been developed that\\ninvolve the addition of activating agents to stabilize the\\nresulting anionic charge. To date, there are only a few\\nexamples of these reactions in a catalytic asymmetric version,\\nand the reaction scope is very limited. Inspired by these\\nstudies, we envisaged that the alkyl nucleophiles generated\\nin situ from 2-methylpyridines might be compatible with\\niridium-catalyzed asymmetric allylic substitution reactions.\\nUnder iridium catalysis, complementary selectivity and sub-\\nstrate scope would facilitate the utility of the methods in the\\nsynthesis of natural products and bioactive molecules\\n(Figure 1).[9] Herein, we describe the first iridium-catalyzed\\nasymmetric allylic substitution reaction with highly unstabi-\\nlized alkyl nucleophiles derived in situ from 2-methylpyri-\\ndines.\\n\\nFigure 1. Selected bioactive compounds and natural products that\\ncontain the circled structural motif.',\n",
       "  'We began our study by testing the reaction of tert-butyl\\ncinnamyl carbonate with the complex generated in situ from\\n2-methylpyridine and BF3·OEt2 with different\\nligands\\n(Table 1). It\\nis proposed that BF3·OEt2 coordinates to\\npyridine nitrogen to increase the acidity of the proton of the\\nmethyl group and stabilize the resulting carboanion. The\\nFeringa ligand and its variants (L1, L2, L3, and L4) afforded\\nthe desired product in varied yields and enantioselectivity. To\\nour delight, the Alexakis ligand L2 gave 3 a in 71 % yield and\\n95 % ee. The reactions with L5 or L6 only led to a trace\\namount of product. The reaction outcomes of ligands L7, L8,\\nand L9 were unsatisfactory, and only L7, which contains\\na methoxy group, afforded 3 a in 27 % yield and 84 % ee.\\nFurther optimization of the reaction conditions revealed that\\nLiHMDS is the optimal base, but multiple equivalents\\n\\nAngew. Chem. Int. Ed. 2017, 56, 1 – 5\\n\\n(cid:2) 2017 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim\\n\\nThese are not the final page numbers! (cid:2)\\n\\n1\\n\\n(cid:2)\\n\\n\\x0cCommunications\\n\\nAngewandte\\n\\nChemie\\n\\nTable 1: Optimization of the reaction conditions.[a]\\n\\nTable 2: Substrate scope with respect to the allyl carbonates.[a,b,c]\\n\\nEntry\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10[d]\\n11[e]\\n12[f ]\\n\\nL\\n\\nL1\\nL2\\nL3\\nL4\\nL5\\nL6\\nL7\\nL8\\nL9\\nL2\\nL2\\nL2\\n\\nYield [%][b]\\n\\nee [%][c]\\n\\n23\\n71\\n20\\n42\\ntrace\\ntrace\\n27\\ntrace\\ntrace\\n73\\n71\\n54\\n\\n84\\n95\\n80\\n(cid:2)77\\n–\\n–\\n84\\n–\\n–\\n97\\n97\\n97'],\n",
       " 19)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "\n",
    "recursive_chunks = []\n",
    "for doc in docs:\n",
    "    recursive_chunks.extend(text_splitter.split_text(doc))\n",
    "recursive_chunks[:5], len(recursive_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 将分块存入向量数据库中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 嵌入模型选择\n",
    "##### 向量数据库选择\n",
    "##### 持久化到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HHH\\.conda\\envs\\chemical_rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\HHH\\.conda\\envs\\chemical_rag\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\HHH\\.conda\\envs\\chemical_rag\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Users\\HHH\\.conda\\envs\\chemical_rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
    "\n",
    "# 保存到当前根目录的vector_db文件夹下\n",
    "kb_name = \"vector_mytry_1\"\n",
    "persist_directory = os.path.join(os.getcwd(), kb_name)\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=recursive_chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step4 检索chain构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词表扩充 sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup QnA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import sentencepiece as spm\n",
    "\n",
    "input_files = [\n",
    "    \"./data/corpus.txt\",\n",
    "]\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_files,\n",
    "    model_prefix='tokenizer_spm_model',\n",
    "    vocab_size=5000,\n",
    "    character_coverage=0.9995, # 字符集丰富的中文、日文，设置为0.9995\n",
    "    model_type='bpe', # unigram、bpe、word和char\n",
    "    max_sentence_length=2048, # UTF-8中一个汉字3个字节，最大长度为2048字节\n",
    ")\n",
    "\n",
    "# tokenizer_spm_model.model 为模型文件\n",
    "# tokenizer_spm_model.vocab 为词表文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合并llm和领域词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "from transformers import LlamaTokenizer\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "\n",
    "llama2_tokenizer_dir = \"llama2_tokenizer/tokenizer.model\"\n",
    "llama2_tokenizer = LlamaTokenizer.from_pretrained(llama2_tokenizer_dir)\n",
    "\n",
    "chinese_sp_model = spm.SentencePieceProcessor()\n",
    "chinese_sp_model_file = \"tokenizer_spm_model.model\"\n",
    "chinese_sp_model.Load(chinese_sp_model_file)\n",
    "\n",
    "llama2_spm = sp_pb2_model.ModelProto()\n",
    "llama2_spm.ParseFromString(llama2_tokenizer.sp_model.serialized_model_proto())\n",
    "\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n",
    "\n",
    "# add chinese tokens to llama2 tokenizer\n",
    "llama_spm_tokens_set = set(p.piece for p in llama2_spm.pieces)\n",
    "for p in chinese_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama2_spm.pieces.append(new_p)\n",
    "f\"New model pieces: {len(llama2_spm.pieces)}\"\n",
    "\n",
    "## save\n",
    "output_sp_dir = 'llama2_chinese'\n",
    "os.makedirs(output_sp_dir, exist_ok=True)\n",
    "with open(output_sp_dir + '/chinese_llama2.model', 'wb') as f:\n",
    "    f.write(llama2_spm.SerializeToString())\n",
    "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir + '/chinese_llama2.model')\n",
    "output_hf_dir = 'llama2_chinese'\n",
    "os.makedirs(output_hf_dir, exist_ok=True)\n",
    "tokenizer.save_pretrained(output_hf_dir)\n",
    "f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\"\n",
    "\n",
    "# /llama2_chinese\n",
    "# /chinese_llama2.model /special_tokens_map.json /tokenizer_config.json /tokenizer.model\n",
    "# /llama2_tokenizer/tokenizer.model\n",
    "\n",
    "# test\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama2_tokenizer_dir)\n",
    "chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
    "tokenizer.all_special_ids\n",
    "tokenizer.all_special_tokens\n",
    "tokenizer.special_tokens_map\n",
    "text = \"白日依山尽，黄河入海流。欲穷千里目，更上一层楼\"\n",
    "f\"test text: {text}\"\n",
    "f\"tokenized by llama2 tokenizer: {llama_tokenizer.tokenize(text)}\"\n",
    "f\"tokenized by chinese llama tokenizer: {chinese_llama_tokenizer.tokenize(text)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query rewrite  构建重写器来重写搜索查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "rewrite_template = \"\"\"\n",
    "    Provide a better search query for \\\n",
    "    web search engine to answer the given question, end \\\n",
    "    the queries with '**'. Question: \\\n",
    "    {x} Answer:\n",
    "\"\"\"\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(rewrite_template)\n",
    "\n",
    "def _parse(text):\n",
    "    return text.strip(\"**\")\n",
    "\n",
    "rewriter = rewrite_prompt | llm | StrOutputParser() | _parse\n",
    "\n",
    "base_template = \"\"\"\n",
    "    Answer the users question based only on the following context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "base_prompt = ChatPromptTemplate(template=base_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "def retriever(query):\n",
    "    return search.run(query)\n",
    "\n",
    "rewrite_retrieve_read_chain = (\n",
    "    {\n",
    "        \"context\": { \"x\": RunnablePassthrough() } | rewriter | retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | base_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个块长度很长，且包含有不相干的信息；使用LLM从检索到的文档中删除那些不相关的段落。\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline, EmbeddingsFilter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embedding)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embedding,similarity_threshold=0.75)\n",
    "pipeline_compressor = DocumentCompressorPipeline(transformers=[splitter, redundant_filter, relevant_filter])\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    doc_strings = [doc.page_content for doc in docs]\n",
    "    return \"\\n\\n\".join(doc_strings)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": { \"x\": RunnablePassthrough() } | rewriter | compression_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | base_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What did the author do growing up?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare multi query retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectordb.as_retriever(), llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"\n",
    "    Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    {context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def format_docs (docs):\n",
    "    doc_strings = [doc.page_content for doc in docs]\n",
    "    return \"\\n\\n\".join(doc_strings)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_from_llm | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step5 Ragas效果评估比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = []\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "text_splitter_chunks = text_splitter.create_documents([d.page_content for d in documents])\n",
    "\n",
    "questions = []\n",
    "ground_truths_semantic = []\n",
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "question_template = \"\"\"\\\n",
    "    Given the following context, answer the question using only the context. If the answer is not contained within the context, say \"I don't know.\"\n",
    "    Context: {context}\n",
    "\"\"\"\n",
    "question_prompt = ChatPromptTemplate.from_template(question_template)\n",
    "\n",
    "ground_truth_template = \"\"\"\\\n",
    "    Given the following context, answer the question using only the context. If the answer is not contained within the context, say \"I don't know.\"\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "ground_truth_prompt = ChatPromptTemplate.from_template(ground_truth_template)\n",
    "\n",
    "question_chain = question_prompt | llm | StrOutputParser()\n",
    "ground_truth_chain = ground_truth_prompt | llm | StrOutputParser()\n",
    "\n",
    "for chunk in text_splitter_chunks[10:20]:\n",
    "    questions.append(question_chain.invoke({\"context\": chunk.page_content}))\n",
    "    contexts.append(chunk.page_content)\n",
    "    ground_truths_semantic.append(ground_truth_chain.invoke({\"context\": chunk.page_content, \"question\": questions[-1]}))\n",
    "    answers.append(text_splitter_chunks.invoke(questions[-1]))\n",
    "\n",
    "#  将生成的内容格式化为HuggingFace数据集格式\n",
    "from datasets import load_dataset, Dataset\n",
    "qagc_list = []\n",
    "\n",
    "for question, answer, context, ground_truth in zip(questions, answers, contexts, ground_truths_semantic):\n",
    "    qagc_list.append({\"question\": question, \"answer\": answer, \"context\": context, \"ground_truth\": ground_truth})\n",
    "\n",
    "eval_dataset = Dataset.from_list(qagc_list)\n",
    "# Dataset({\n",
    "#     features: ['question', 'answer', 'context', 'ground_truth'],\n",
    "#     num_rows: 100\n",
    "# })\n",
    "\n",
    "# 实施Ragas指标并评估我们创建的数据集。\n",
    "from ragas.metrics import answer_relavancy, faithfulness, context_recall, context_precision\n",
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[answer_relavancy, faithfulness, context_recall, context_precision],\n",
    "    llm=llm,\n",
    "    embeddings=embedding,\n",
    "    raise_exceptions=False\n",
    ")\n",
    "\n",
    "# {'context_precision': 1.0000, 'faithfulness': 0.8857, 'answer_relevancy': 0.9172, 'context_recall': 1.0000}\n",
    "result_df = result.to_pandas()\n",
    "result_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemical_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
