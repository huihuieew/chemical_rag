{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM之RAG实战（二十八）| 探索RAG query重写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、假设文档嵌入（HyDE）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'your key'\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'your dir path'\n",
    "\n",
    "documents = SimpleDirectoryReader(dir_path).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"what did paul graham do after going to RISD\"\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query_str)\n",
    "\n",
    "print('--' * 50)\n",
    "print('Base query:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, transform=hyde)\n",
    "response = hyde_query_engine.query(query_str)\n",
    "\n",
    "print('--' * 50)\n",
    "print('After HyDEQueryTransform:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyDEQueryTransform(BaseQueryTransform):\n",
    "    \"\"\"Hypothetical Document Embeddings (HyDE) query transform.\n",
    "\n",
    "    It uses an LLM to generate hypothetical answer(s) to a given query,\n",
    "    and use the resulting documents as embedding strings.\n",
    "\n",
    "    As described in `[Precise Zero-Shot Dense Retrieval without Relevance Labels]\n",
    "    (https://arxiv.org/abs/2212.10496)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[LLMPredictorType] = None,\n",
    "        hyde_prompt: Optional[BasePromptTemplate] = None,\n",
    "        include_original: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize HyDEQueryTransform.\n",
    "\n",
    "        Args:\n",
    "            llm_predictor (Optional[LLM]): LLM for generating\n",
    "                hypothetical documents\n",
    "            hyde_prompt (Optional[BasePromptTemplate]): Custom prompt for HyDE\n",
    "            include_original (bool): Whether to include original query\n",
    "                string as one of the embedding strings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self._llm = llm or Settings.llm\n",
    "        self._hyde_prompt = hyde_prompt or DEFAULT_HYDE_PROMPT\n",
    "        self._include_original = include_original\n",
    "    def _get_prompts(self) -> PromptDictType:\n",
    "        \"\"\"Get prompts.\"\"\"\n",
    "        return {\"hyde_prompt\": self._hyde_prompt}\n",
    "\n",
    "    def _update_prompts(self, prompts: PromptDictType) -> None:\n",
    "        \"\"\"Update prompts.\"\"\"\n",
    "        if \"hyde_prompt\" in prompts:\n",
    "            self._hyde_prompt = prompts[\"hyde_prompt\"]\n",
    "            \n",
    "\n",
    "    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n",
    "        \"\"\"Run query transform.\"\"\"\n",
    "        # TODO: support generating multiple hypothetical docs\n",
    "        query_str = query_bundle.query_str\n",
    "        hypothetical_doc = self._llm.predict(self._hyde_prompt, context_str=query_str)\n",
    "        embedding_strs = [hypothetical_doc]\n",
    "        if self._include_original:\n",
    "            embedding_strs.extend(query_bundle.embedding_strs)\n",
    "\n",
    "        # The following three lines contain the added debug statements.\n",
    "        print('-' * 100)\n",
    "        print(\"Hypothetical doc:\")\n",
    "        print(embedding_strs)\n",
    "\n",
    "        return QueryBundle(\n",
    "            query_str=query_str,\n",
    "            custom_embedding_strs=embedding_strs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYDE_TMPL = (\n",
    "    \"please write a passage to answer the question\\n\"\n",
    "    \"try to include as many key details as possible\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    'passage:\\n'\n",
    ")\n",
    "\n",
    "DEFAULT_HYDE_PROMPT = PromptTemplate(template=HYDE_TMPL, \n",
    "                                     prompt_type=PromptType.SUMMARY,\n",
    "                                     input_variables=[\"context_str\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、重写-检索-读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
    "\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_template = \"\"\"\n",
    "    Answer the users question based only on the following context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "base_prompt = ChatPromptTemplate(template=base_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "search = DuckDuckGoSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(query):\n",
    "    return search.run(query)\n",
    "\n",
    "def june_print(msg, res):\n",
    "    print('--' * 50)\n",
    "    print(msg)\n",
    "    print(res)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | base_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "\n",
    "june_print(\n",
    "    \"the result of query\",\n",
    "    chain.invoke(query)\n",
    ")\n",
    "\n",
    "june_print(\n",
    "    \"the result of the searched contexts\",\n",
    "    retriever(query)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现在就开始构建重写器来重写搜索查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_template = \"\"\"\n",
    "    Provide a better search query for \\\n",
    "    web search engine to answer the given question, end \\\n",
    "    the queries with '**'. Question: \\\n",
    "    {x} Answer:\n",
    "\"\"\"\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(rewrite_template)\n",
    "\n",
    "def _parse(text):\n",
    "    return text.strip(\"**\")\n",
    "\n",
    "rewriter = rewrite_prompt | ChatOpenAI(temperature=0) | StrOutputParser() | _parse\n",
    "june_print(\n",
    "    'rewritten query:',\n",
    "    rewriter.invoke({'x': query})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造rewrite_retrieve_read_chain并利用重写后的查询。\n",
    "\n",
    "rewrite_retrieve_read_chain = (\n",
    "    {\n",
    "        \"context\": { \"x\": RunnablePassthrough() } | rewriter | retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | base_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "june_print(\n",
    "    'the result of the rewrite_retrieve_read_chain:',\n",
    "    rewrite_retrieve_read_chain.invoke(query)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、Step-Back提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR_API_KEY'\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###     构建一个链并执行原始查询：\n",
    "def june_print(msg, res):\n",
    "    print('--' * 50)\n",
    "    print(msg)\n",
    "    print(res)\n",
    "\n",
    "question = 'was chatgpt around while trump was president?'\n",
    "\n",
    "base_prompt_template = \"\"\"\n",
    "    you are an expert of world knowledge. I am going to ask you a question.\n",
    "    {normal_context}\n",
    "    Original Qestion: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "base_prompt = ChatPromptTemplate(template=base_prompt_template, input_variables=['normal_context', 'question'])\n",
    "\n",
    "search = DuckDuckGoSearchAPIWrapper(max_results=4)\n",
    "\n",
    "def retriever(query):\n",
    "    return search.run(query)\n",
    "\n",
    "base_chain = (\n",
    "    {\n",
    "        \"normal_context\": RunnableLambda(lambda x: x['question']) | retriever,\n",
    "        \"question\": lambda x: x['question']\n",
    "    }\n",
    "    | base_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "june_print('the searched contexts of the original question:', retriever(question))\n",
    "june_print('the result of base_chain:', base_chain.invoke({'question': question}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few shot examples\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"could the members of the police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of the police do?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"what is the purpose of the police?\",\n",
    "        \"output\": \"what is the purpose of the police?\"\n",
    "    }\n",
    "]\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates from English to Pig Latin.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"\",\n",
    "    suffix=\"\",\n",
    ")\n",
    "step_back_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates from English to Pig Latin.\"),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{question}\"),\n",
    "])\n",
    "\n",
    "step_back_question_chain = step_back_prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "june_print(\n",
    "    'the step back question:',\n",
    "    step_back_question_chain.invoke({\"question\": question})\n",
    ")\n",
    "june_print(\n",
    "    'the searched contexts of the step back question:',\n",
    "    retriever(step_back_question_chain.invoke({\"question\": question}))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = \"\"\"\n",
    "You are a helpful assistant. Given the following extracted parts of a long document and a question, create a final answer. \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "If the answer is not contained within the text below, say \\\"I don't know\\\"\n",
    "\n",
    "{normal_context}\n",
    "{step_back_context}\n",
    "\n",
    "Original question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(\n",
    "    template=response_prompt_template,\n",
    "    input_variables=[\"normal_context\", \"step_back_context\", \"question\"],\n",
    ")\n",
    "\n",
    "step_back_chain = (\n",
    "    {\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        \"step_back_context\": step_back_question_chain | retriever,\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "june_print(\n",
    "    \"Step back chain created. Now let's try it out!\",\n",
    "    step_back_chain.invoke({\"question\": question})\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemical_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
