{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# model_path = r\"C:\\Users\\HHH\\.cache\\modelscope\\hub\\llm-research\\meta-llama-3___1-8b\"\n",
    "model_path = r\"C:/Users/HHH/.cache/modelscope/hub/llm-research/meta-llama-3___1-8b\"\n",
    "# 加载本地模型\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_path, legacy=False)\n",
    "# 加载模型到GPU上\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "prompt = \"你好，Llama3！我是人类，请随意与我聊天。\"\n",
    "\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "# print(\"input_ids:\", input_ids)\n",
    "# response = model.generate(input_ids, max_new_tokens=128, do_sample=True, temperature=0.9, top_p=0.7, top_k=40)\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"请用中文回答。hello, world! is a common greeting in English. In Chinese, it is usually said as nǐ hǎo, shìjiè! nǐ hǎo, shìjiè! is a phrase that is often used to express one's happiness and excitement.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "model_path = \"C:/Users/HHH/.cache/modelscope/hub/llm-research/meta-llama-3___1-8b\"\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "text = \"请用中文回答。hello, world!\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids, max_length=100)\n",
    "outputs\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step chatglm-6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMTokenizer(name_or_path='THUDM/chatglm3-6b', vocab_size=64798, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t64790: AddedToken(\"[gMASK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t64792: AddedToken(\"sop\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t64795: AddedToken(\"<|user|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t64796: AddedToken(\"<|assistant|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
      "- configuration_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
      "- modeling_chatglm.py\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from model_factory import yi_llm\n",
    "# import PyPDF2\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ab78b1a21a614ba1ade36ba88b172893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am an AI assistant developed by 01.AI, designed to be a helpful resource for you, capable of answering questions and offering insightful information across a wide range of topics. How may I be of service to you today?\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key='ab78b1a21a614ba1ade36ba88b172893',\n",
    "    base_url='https://api.lingyiwanwu.com/v1'\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"yi-large\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hi, who are you?\"}]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemical_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
